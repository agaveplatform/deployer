#!/bin/bash

# Based on https://gist.github.com/2206527

# Basic variables
mysqlpass="{{ mysql_root_password }}"
bucket="s3://agavedailybackups"

# Timestamp (sortable AND readable)
stamp=`date +"%Y-%m-%d"`-"{{ tenant_id }}-auth"

# List all the databases
databases=`mysql -u {{ mysql_root_user }}  -p$mysqlpass -e "SHOW DATABASES;" | tr -d "| " | grep -v "\(Database\|information_schema\|performance_schema\|mysql\|test\)"`

# Feedback
echo -e "Dumping to \e[1;32m$bucket/$stamp/\e[00m"

# Loop the databases
for db in $databases; do

  # Define our filenames
  basefilename="$stamp-$db"
  tmpdir="/tmp/$basefilename"
  object="$bucket/$stamp/${basefilename}.tgz"

  # Feedback
  echo -e "\e[1;34m$db\e[00m"

  # Dump and zip
  echo -e "  creating \e[0;35m${tmpdir}\e[00m"
  mkdir -p "$tmpdir"

  echo -e "  dumping structure \e[0;35m$tmpdir/${basefilename}.tables.sql\e[00m"
  mysqldump -u {{ mysql_root_user }} -h 172.17.42.1 -p$mysqlpass --force --opt --databases "$db" --no-data > "$tmpdir/${basefilename}.tables.sql"
  echo -e "  dumping structure \e[0;35m$tmpdir/${basefilename}.data.sql\e[00m"
  mysqldump -u {{ mysql_root_user }} -h 172.17.42.1 -p$mysqlpass --force --opt --databases "$db" > "$tmpdir/${basefilename}.data.sql"


  tar -czf  "${tmpdir}.tgz" "${tmpdir}"

  # Upload
  echo -e "  uploading..."
  s3cmd put "${tmpdir}.tgz" "$object"

  # Delete
  echo -e "  cleaning up $tmpdir..."
  rm -rf "${tmpdir}" "${tmpdir}.tgz"

done;

# Jobs a goodun
echo -e "\e[1;32mJobs a goodun\e[00m"

